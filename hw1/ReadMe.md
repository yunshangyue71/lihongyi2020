#介绍
这个的评估指标，固定了。主要是学习特征工程的东西，换句话说，选取那些特征进行拟合会更好  
这里我们追求性能高一些, 对训练的时间不做要求

# 使用所有数据
## lr = 2
3000 次会收敛，  
0 train_loss: 29.2785, val_loss: 111.6337  
1000 train_loss: 6.2284, val_loss: 4.9213  
2000 train_loss: 6.1083, val_loss: 4.8645  
3000 train_loss: 6.0710, val_loss: 4.8537  
23000 train_loss: 6.0427, val_loss: 4.8830  
24000 train_loss: 6.0426, val_loss: 4.8830  
25000 train_loss: 6.0425, val_loss: 4.8831  
26000 train_loss: 6.0424, val_loss: 4.8831  
## lr = 0.5
14000 收敛， 4.88  
14000 train_loss: 6.0435, val_loss: 4.8819  
15000 train_loss: 6.0433, val_loss: 4.8822  
16000 train_loss: 6.0431, val_loss: 4.8824  
## lr = 0.1
60000万次会收敛， 损失为4.8， 效果好一丢丢  
54000 train_loss: 6.0505, val_loss: 4.8252  
55000 train_loss: 6.0497, val_loss: 4.8282  
56000 train_loss: 6.0490, val_loss: 4.8311  
57000 train_loss: 6.0484, val_loss: 4.8339
58000 train_loss: 6.0478, val_loss: 4.8365  
59000 train_loss: 6.0473, val_loss: 4.8390  
60000 train_loss: 6.0469, val_loss: 4.8414  

# 仅仅使用1 观测物的数据进行训练看看效果
仅仅使用Pm2.5 和使用全部的数据差不多，其他的数据起到什么作用
0. 19000 train_loss: 17.9126, val_loss: 11.6790  
1. 19000 train_loss: 17.2013, val_loss: 11.2339  
2. 19000 train_loss: 16.2664, val_loss: 11.4508  
3. 19000 train_loss: 16.4707, val_loss: 11.1024  
4. 19000 train_loss: 17.4704, val_loss: 11.9682  
5. 19000 train_loss: 15.3837, val_loss: 9.9617  
6. 19000 train_loss: 15.6299, val_loss: 10.2667  
7. 19000 train_loss: 16.8695, val_loss: 11.7326  
8. 19000 train_loss: 11.0801, val_loss: 8.3896  
9. 19000 train_loss: 6.5965, val_loss: 4.8105  这个就是PM2.5
10. 19000 train_loss: 17.9935, val_loss: 12.3202  
11. 19000 train_loss: 16.7448, val_loss: 13.5152  
12. 19000 train_loss: 15.5925, val_loss: 12.2887  
13. 19000 train_loss: 15.8922, val_loss: 10.5031  
14. 19000 train_loss: 17.6352, val_loss: 11.7744  
15. 19000 train_loss: 17.7750, val_loss: 11.6798  
16. 19000 train_loss: 18.0219, val_loss: 11.7681  
17. 19000 train_loss: 18.1538, val_loss: 11.8055 

# 看看非PM2.5的权重 
0 9 10 气温、 PM2.5 降雨对这个影响比较大  
温度对分子的扩散、降雨将粉尘击落  
下面是将w 球绝对值后的sum
## 0 .27.926428098376462   
 [21.71155191  0.55523008  0.60805416  1.07499163  1.94757362  0.17740271  0.36537755  0.20290402  1.28334241]   
## 1. 3.9890011172616693 
 [2.31090977 0.07037972 0.35243873 0.06484142 0.17653036 0.01313501 0.30730706 0.1122126  0.58124646]  
## 2. 1.097623449381577 
 [0.56121453 0.04471285 0.02078946 0.03477063 0.03536904 0.09400048 0.04454368 0.1539176  0.10830519]  
## 3. 3.3796637814728703 
 [0.40109895 0.00097076 0.48408198 0.29142518 0.13028642 0.36743292 0.91359799 0.44381569 0.34695389]  
## 4. 2.3769515452318397 
 [0.06961889 0.26901764 0.21928836 0.16676626 0.66468587 0.11182449 0.35336805 0.01432811 0.50805387]  
## 5. 6.108288932753145 
 [0.22736026 0.03720624 0.05195886 0.11564739 1.42382918 0.611769 0.31080953 0.64350141 2.68620705]  
## 6. 7.556248770102728 
 [1.0025321  0.04100068 0.93000854 0.17447015 1.73512146 0.35400775 0.80651642 0.06075224 2.45183941]  
## 7. 3.727206099284582 
 [1.06409355 0.06862943 0.03462251 0.46289255 0.15141407 0.38025093 0.10867801 0.92484805 0.531777  ]  
## 8. 10.7092921294431 
 [2.53002443 0.35108303 0.23107591 1.20173398 1.79949501 0.26448414 2.22146497 1.72747725 0.38245341]  
## 9. 27.53465345665186 
 [0.49932414 0.8113469  0.0440246  3.64530309 3.9290339  0.80459133 8.29453144 9.08510227 0.42139579]  
## 10. 16.272176999215986 
 [15.63088304  0.1239311   0.03659269  0.08829398  0.02500107  0.10289125  0.1010531   0.08518331  0.07834745]  
## 11. 5.794769315671086 
 [0.23202207 0.48580911 0.57091406 0.10238826 0.11510521 1.45176307 1.13115334 1.69272863 0.01288556]  
## 12. 3.5770928785925022 
 [1.39713158 0.63263005 0.75854305 0.21117223 0.13820063 0.03896362 0.03034698 0.06838173 0.301723  ]  
## 13. 2.8768155312421 
 [0.23681806 0.08378765 0.44196763 0.03707151 0.08889815 0.03310125 0.94247249 0.56261612 0.45008267]  
## 14. 1.427862090922377 
 [0.2571871  0.02034423 0.34981358 0.16579304 0.1265542  0.11235358 0.01471349 0.31410875 0.06699411]  
## 15. 1.265990042001083 
 [0.10985331 0.30002707 0.14053214 0.21955989 0.15750575 0.10279497 0.00450211 0.02645948 0.20475534]  
## 16. 1.3556864467201064 
 [0.06201147 0.11182232 0.26350807 0.22659695 0.07380847 0.16228578 0.16160359 0.13555729 0.15849251]  
## 17. 1.6718347024137263 
0.1829115  0.07020403 0.09848836 0.09918342 0.14917349 0.00240299 0.39092999 0.39619434 0.28234659  
## 18. bias 
0.2016253

